{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtvsFeXD906NjvVypgEx9a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Codewith-NandaYadav09/Hands_on_LLM/blob/main/EDDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# file: data_utils.py\n",
        "# author: songyouwei <youwei0314@gmail.com>\n",
        "# Copyright (C) 2018. All Rights Reserved.\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertConfig, BertTokenizer, BertModel, \\\n",
        "                         RobertaConfig, RobertaTokenizer, RobertaModel, \\\n",
        "                         AlbertTokenizer, AlbertConfig, AlbertModel, \\\n",
        "                         AutoTokenizer\n",
        "import csv\n",
        "import random\n",
        "\n",
        "def build_tokenizer(fnames, max_seq_len, dat_fname, add_num):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading tokenizer:', dat_fname)\n",
        "        tokenizer = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        text = ''\n",
        "        for fname in fnames:\n",
        "            if type(fname) == list:\n",
        "                for sign_fname in fname:\n",
        "                    if 'add' not in sign_fname:\n",
        "                        _add_num = 1000000\n",
        "                    else:\n",
        "                        _add_num = add_num\n",
        "                    with open(sign_fname,'r') as f:\n",
        "                        l1s = csv.DictReader(f)\n",
        "                        for l1,_ in zip(l1s,range(_add_num)):\n",
        "                            text += l1['Tweet'] + ' ' + l1['Reason'] + ' ' + l1['Target']\n",
        "            else:\n",
        "                if 'add' not in fname:\n",
        "                    _add_num = 1000000\n",
        "                else:\n",
        "                    _add_num = add_num\n",
        "                with open(fname,'r') as f:\n",
        "                    l1s = csv.DictReader(f)\n",
        "                    for l1,_ in zip(l1s,range(_add_num)):\n",
        "                        text += l1['Tweet'] + ' ' + l1['Reason'] + ' ' + l1['Target']\n",
        "\n",
        "        tokenizer = Tokenizer(max_seq_len)\n",
        "        tokenizer.fit_on_text(text)\n",
        "        pickle.dump(tokenizer, open(dat_fname, 'wb'))\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def _load_word_vec(path, word2idx=None, embed_dim=300):\n",
        "    fin = open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    word_vec = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split()\n",
        "        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]\n",
        "        if word in word2idx.keys():\n",
        "            word_vec[word] = np.asarray(vec, dtype='float32')\n",
        "    return word_vec\n",
        "\n",
        "\n",
        "def build_embedding_matrix(word2idx, embed_dim, dat_fname):\n",
        "    if os.path.exists(dat_fname):\n",
        "        print('loading embedding_matrix:', dat_fname)\n",
        "        embedding_matrix = pickle.load(open(dat_fname, 'rb'))\n",
        "    else:\n",
        "        print('loading word vectors...')\n",
        "        embedding_matrix = np.zeros((len(word2idx) + 2, embed_dim))  # idx 0 and len(word2idx)+1 are all-zeros\n",
        "        fname = './glove.twitter.27B/glove.twitter.27B.' + str(embed_dim) + 'd.txt' \\\n",
        "            if embed_dim != 300 else '/home/dingdaijun/data_list/dingdaijun/glove.42B.300d.txt'\n",
        "        word_vec = _load_word_vec(fname, word2idx=word2idx, embed_dim=embed_dim)\n",
        "        print('building embedding_matrix:', dat_fname)\n",
        "        for word, i in word2idx.items():\n",
        "            vec = word_vec.get(word)\n",
        "            if vec is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = vec\n",
        "        pickle.dump(embedding_matrix, open(dat_fname, 'wb'))\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def pad_and_truncate(sequence, maxlen, dtype='int64', padding='post', truncating='post', value=0):\n",
        "    x = (np.ones(maxlen) * value).astype(dtype)\n",
        "    if truncating == 'pre':\n",
        "        trunc = sequence[-maxlen:]\n",
        "    else:\n",
        "        trunc = sequence[:maxlen]\n",
        "    trunc = np.asarray(trunc, dtype=dtype)\n",
        "    if padding == 'post':\n",
        "        x[:len(trunc)] = trunc\n",
        "    else:\n",
        "        x[-len(trunc):] = trunc\n",
        "    return x\n",
        "\n",
        "\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, max_seq_len, lower=True):\n",
        "        self.lower = lower\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 1\n",
        "\n",
        "    def fit_on_text(self, text):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        words_num = {}\n",
        "        for word in words:\n",
        "            if words_num.get(word) == None:\n",
        "                words_num[word] = 0\n",
        "            words_num[word] += 1\n",
        "        for word in words_num:\n",
        "            if word not in self.word2idx and words_num[word] > 1:\n",
        "                self.word2idx[word] = self.idx\n",
        "                self.idx2word[self.idx] = word\n",
        "                self.idx += 1\n",
        "        print('tokenizer is over:', len(self.word2idx))\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "        if self.lower:\n",
        "            text = text.lower()\n",
        "        words = text.split()\n",
        "        unknownidx = len(self.word2idx)+1\n",
        "        sequence = [self.word2idx[w] if w in self.word2idx else unknownidx for w in words]\n",
        "        if len(sequence) == 0:\n",
        "            sequence = [0]\n",
        "        if reverse:\n",
        "            sequence = sequence[::-1]\n",
        "\n",
        "        return pad_and_truncate(sequence, self.max_seq_len, padding=padding, truncating=truncating)\n",
        "\n",
        "\n",
        "class Tokenizer4Bert:\n",
        "    def __init__(self, max_seq_len, pretrained_name):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_name)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.tokenizer.add_tokens([])\n",
        "\n",
        "    def text_to_sequence(self, text, reverse=False, padding='post', truncating='post'):\n",
        "\n",
        "        if len(text) == 1:\n",
        "            sequence = self.tokenizer.encode_plus(text[0],max_length=self.max_seq_len + add,padding='max_length',truncation=True,return_tensors='pt').values()\n",
        "        else:\n",
        "            sequence = self.tokenizer.encode_plus(text[0],text_pair=text[1],max_length=self.max_seq_len + add,padding='max_length',truncation=True,return_tensors='pt').values()\n",
        "        sequence = [item.squeeze(0) for item in sequence]\n",
        "\n",
        "        if len(sequence[0]) > self.max_seq_len + add:\n",
        "            # print(\"===========it's Prompt==========\")\n",
        "            for index in range(len(sequence)):\n",
        "                sequence[index] = sequence[index][len(sequence[index]) - self.max_seq_len - add:]\n",
        "        return sequence\n",
        "\n",
        "import csv\n",
        "import random\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, fname, tokenizer,opt,tt_model='train',ratio=1):\n",
        "        all_data = []\n",
        "        match = {'AGAINST':0,'FAVOR':1,'NONE':2,'0':0,'1':1,'2':2,'SUPPORT':1,'NEUTRAL':2,'OPPOSED':0}\n",
        "\n",
        "        def deal_data(sign_fname):\n",
        "            len_path_data = sum(1 for _ in open(sign_fname)) - 1\n",
        "            rand_pp = [0] * len_path_data\n",
        "            ran_po = random.sample(range(len_path_data),int(opt.label_ratio * len_path_data))\n",
        "            for i in ran_po:\n",
        "                rand_pp[i] = 1\n",
        "            sall_data = []\n",
        "            if tt_model == 'add':\n",
        "                max_data_len = opt.add_num\n",
        "            else:\n",
        "                max_data_len = len_path_data + 10000\n",
        "            with open(sign_fname,'r',encoding='utf-8') as f:\n",
        "                lines = csv.DictReader(f)\n",
        "                for index,(line,_) in enumerate(zip(lines,range(max_data_len))):\n",
        "                    text = line['Tweet']\n",
        "                    reason = line['Reason']\n",
        "                    if len(reason) <= 1:\n",
        "                        reason = 'empty'\n",
        "                    target = line['Target']\n",
        "\n",
        "                    if tt_model == 'test' or tt_model == 'train':\n",
        "                        polarity_s = match[line['Stance'].upper()]\n",
        "                        polarity = polarity_s\n",
        "                    elif tt_model == 'add':\n",
        "                        polarity_a = match[line['Attitude'].upper()]\n",
        "                        polarity = polarity_a\n",
        "\n",
        "                    if  'bart' in opt.model_name or 'bert' in opt.model_name:\n",
        "                        bert_text = tokenizer.text_to_sequence([text])\n",
        "                        bert_text_target = tokenizer.text_to_sequence([text,target])\n",
        "                        bert_text_reason = tokenizer.text_to_sequence([text, reason])\n",
        "                        bert_reason = tokenizer.text_to_sequence([reason])\n",
        "                        bert_reason_target = tokenizer.text_to_sequence([reason,target])\n",
        "                        # bert_text_reason_target = tokenizer.text_to_sequence([text + '[SEP]' + reason,target])\n",
        "\n",
        "                        if 'bert' in opt.model_name:\n",
        "                            data = {\n",
        "                                'bert_text_inputs': bert_text[0],\n",
        "                                'bert_text_type': bert_text[1],\n",
        "                                'bert_text_mask': bert_text[2],\n",
        "\n",
        "                                'bert_text_target_inputs': bert_text_target[0],\n",
        "                                'bert_text_target_type': bert_text_target[1],\n",
        "                                'bert_text_target_mask': bert_text_target[2],\n",
        "\n",
        "                                'bert_text_reason_inputs': bert_text_reason[0],\n",
        "                                'bert_text_reason_type': bert_text_reason[1],\n",
        "                                'bert_text_reason_mask': bert_text_reason[2],\n",
        "\n",
        "                                'bert_reason_inputs': bert_reason[0],\n",
        "                                'bert_reason_type': bert_reason[1],\n",
        "                                'bert_reason_mask': bert_reason[2],\n",
        "\n",
        "                                'bert_reason_target_inputs': bert_reason_target[0],\n",
        "                                'bert_reason_target_type': bert_reason_target[1],\n",
        "                                'bert_reason_target_mask': bert_reason_target[2],\n",
        "\n",
        "                                # 'bert_text_reason_target_inputs': bert_text_reason_target[0],\n",
        "                                # 'bert_text_reason_target_type': bert_text_reason_target[1],\n",
        "                                # 'bert_text_reason_target_mask': bert_text_reason_target[2],\n",
        "\n",
        "                                'polarity': polarity,\n",
        "                            }\n",
        "                        else:\n",
        "                            data = {\n",
        "                                'bert_text_inputs': bert_text[0],\n",
        "                                'bert_text_mask': bert_text[1],\n",
        "\n",
        "                                'bert_text_target_inputs': bert_text_target[0],\n",
        "                                'bert_text_target_mask': bert_text_target[1],\n",
        "\n",
        "                                'bert_text_reason_inputs': bert_text_reason[0],\n",
        "                                'bert_text_reason_mask': bert_text_reason[1],\n",
        "\n",
        "                                'bert_reason_inputs': bert_reason[0],\n",
        "                                'bert_reason_mask': bert_reason[1],\n",
        "\n",
        "                                'bert_reason_target_inputs': bert_reason_target[0],\n",
        "                                'bert_reason_target_mask': bert_reason_target[1],\n",
        "\n",
        "                                'bert_text_reason_target_inputs': bert_text_reason_target[0],\n",
        "                                'bert_text_reason_target_mask': bert_text_reason_target[1],\n",
        "\n",
        "                                'polarity': polarity,\n",
        "                            }\n",
        "                    else:\n",
        "                        text_target_indices = tokenizer.text_to_sequence(text + ' ' + target)\n",
        "                        text_reason_indices = tokenizer.text_to_sequence(text + ' ' + reason)\n",
        "                        text_indices = tokenizer.text_to_sequence(text)\n",
        "                        reason_indices = tokenizer.text_to_sequence(reason)\n",
        "                        if reason_indices[0] == 0:\n",
        "                            print(reason,len(reason))\n",
        "                            assert False\n",
        "                        reason_target_indices = tokenizer.text_to_sequence(reason + ' ' + target)\n",
        "                        text_reason_target_indices = tokenizer.text_to_sequence(text + ' ' + reason + ' ' + target)\n",
        "                        data = {\n",
        "                            'text_target_indices': text_target_indices,\n",
        "                            'text_reason_indices': text_reason_indices,\n",
        "                            'text_reason_target_indices': text_reason_target_indices,\n",
        "                            'text_indices': text_indices,\n",
        "                            'reason_indices': reason_indices,\n",
        "                            'reason_target_indices': reason_target_indices,\n",
        "\n",
        "                            'polarity': polarity,\n",
        "                        }\n",
        "                    sall_data.append(data)\n",
        "            return sall_data\n",
        "\n",
        "\n",
        "        if type(fname) == list:\n",
        "            for sign_fname in fname:\n",
        "                sall_data = deal_data(sign_fname)\n",
        "                # all_data.extend(sall_data)\n",
        "                all_data.extend(random.sample(sall_data,int(ratio * len(sall_data))))\n",
        "        else:\n",
        "            sall_data = deal_data(fname)\n",
        "            # print('ratio * len(sall_data)',ratio * len(sall_data),ratio,len(sall_data))\n",
        "            # all_data.extend(sall_data)\n",
        "            all_data.extend(random.sample(sall_data,int(ratio * len(sall_data))))\n",
        "\n",
        "        self.data = all_data\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "YoX-xPH3WOi1",
        "outputId": "fbff9e99-3a8d-40c0-8806-48493234c17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nmodule 'sympy' has no attribute 'core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfx_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aot_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/functional_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sparse_any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from torch.fx.experimental.symbolic_shapes import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdefinitely_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMinMaxBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLatticeOp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moriginal_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0massumptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36mMinMaxBase\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     ) -> Optional[set[sympy.core.symbol.Symbol]]:\n\u001b[0m\u001b[1;32m    636\u001b[0m         \"\"\"\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'sympy' has no attribute 'core'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c01ae3a0d0ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                          \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                          \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlbertModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nmodule 'sympy' has no attribute 'core'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1tmrUV3NWIU_",
        "outputId": "07d15c27-27b7-4a98-dda0-aae66fd8de94"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5840f1eed8d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocaltime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                          \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                          \u001b[0mAlbertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlbertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAlbertModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfx_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aot_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcapture_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoggingTensorMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_functorch/_aot_autograd/functional_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionalTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sparse_any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from torch.fx.experimental.symbolic_shapes import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdefinitely_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msym_eq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mCeilToInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_degree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpquo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexquo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf_gcdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcdex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/polys/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m         GroebnerBasis, poly)\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m from .polyfuncs import (symmetrize, horner, interpolate,\n\u001b[0m\u001b[1;32m     80\u001b[0m         rational_interpolate, viete)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/polys/polyfuncs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyoptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallowed_flags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolytools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpoly_from_expr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPoly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from sympy.polys.specialpolys import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     symmetric_poly, interpolating_poly)\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/polys/specialpolys.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleton\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntheory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnextprime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m from sympy.polys.densearith import (\n\u001b[1;32m      9\u001b[0m     \u001b[0mdmp_add_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmp_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmp_mul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdmp_sqr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/ntheory/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrandprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSieve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msieve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimorial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompositepi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprimetest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misprime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_gaussian_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_mersenne_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfactor_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivisors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproper_divisors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactorint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplicity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmultiplicity_in_factorial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperfect_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpollard_pm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpollard_rho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprimefactors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/ntheory/factor_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilldedent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mecm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ecm_one_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy\n",
        "import csv\n",
        "from sklearn import metrics\n",
        "from time import strftime, localtime\n",
        "\n",
        "from transformers import BertConfig, BertTokenizer, BertModel, \\\n",
        "                         RobertaConfig, RobertaTokenizer, RobertaModel, \\\n",
        "                         AlbertTokenizer, AlbertConfig, AlbertModel,AdamW, \\\n",
        "                         AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "\n",
        "from data_utils import build_tokenizer, build_embedding_matrix, Tokenizer4Bert, ABSADataset\n",
        "from models import LSTM, IAN, MemNet, RAM, TD_LSTM, TC_LSTM, Cabasc, ATAE_LSTM, TNet_LF, AOA, MGAN, ASGCN\n",
        "from models.bert_spc import BERT_SPC\n",
        "from models.bert_rn import BERT_RN\n",
        "from transformers import logging as tlog\n",
        "\n",
        "tlog.set_verbosity_warning()\n",
        "tlog.set_verbosity_error()\n",
        "\n",
        "BASEPATH = ''\n",
        "\n",
        "def get_logger(filename, verbosity=1, name=None):\n",
        "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
        "    formatter = logging.Formatter(\n",
        "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
        "    )\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(level_dict[verbosity])\n",
        "\n",
        "    fh = logging.FileHandler(filename, \"a+\")\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    sh = logging.StreamHandler()\n",
        "    sh.setFormatter(formatter)\n",
        "    logger.addHandler(sh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class Instructor:\n",
        "    def __init__(self, opt):\n",
        "        self.opt = opt\n",
        "        if '-' in self.opt.dataset and self.opt.dataset[-1] != '-':\n",
        "            log_path = f'../log/{self.opt.model_name}/cross/'\n",
        "        elif self.opt.dataset[-1] == '-' or 'vast' in self.opt.dataset:\n",
        "            log_path = f'../log/{self.opt.model_name}/zero/'\n",
        "        else:\n",
        "            log_path = f'../log/{self.opt.model_name}/in-domain/'\n",
        "        if not os.path.exists(log_path):\n",
        "            os.makedirs(log_path)\n",
        "        logger = get_logger(f'{log_path}{self.opt.dataset}_{self.opt.input_type}_{self.opt.add_num}_{self.opt.label_ratio}%_{self.opt.FAD_ratio}%_{self.opt.RAD_ratio}%.log',name='normal')\n",
        "        self.logger = logger\n",
        "        best_logger = get_logger(f'{log_path}best_{self.opt.dataset}_{self.opt.input_type}_{self.opt.add_num}_{self.opt.label_ratio}%_{self.opt.FAD_ratio}%_{self.opt.RAD_ratio}%.log',name='best')\n",
        "        self.best_logger = best_logger\n",
        "\n",
        "        if 'bert' in opt.model_name:\n",
        "            bert = AutoModel.from_pretrained(opt.pretrained_name)\n",
        "            tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.pretrained_name)\n",
        "            if opt.model_name == 'bert_rn':\n",
        "                bert2 = AutoModel.from_pretrained(opt.pretrained_name)\n",
        "                self.model = opt.model_class(bert,bert2, opt).to(opt.device)\n",
        "            else:\n",
        "                self.model = opt.model_class(bert, opt).to(opt.device)\n",
        "        elif 'bart' in opt.model_name:\n",
        "            tokenizer = Tokenizer4Bert(opt.max_seq_len, opt.pretrained_name)\n",
        "            self.model = opt.model_class(opt).to(opt.device)\n",
        "        else:\n",
        "            tokenizer = build_tokenizer(\n",
        "                fnames=[opt.dataset_file['train'], opt.dataset_file['test'], opt.dataset_file['front_add'], opt.dataset_file['reverse_add']],\n",
        "                max_seq_len=opt.max_seq_len,\n",
        "                dat_fname='../dat/{0}_{1}_tokenizer.dat'.format(opt.dataset,opt.add_num),\n",
        "                add_num = opt.add_num)\n",
        "            embedding_matrix = build_embedding_matrix(\n",
        "                word2idx=tokenizer.word2idx,\n",
        "                embed_dim=opt.embed_dim,\n",
        "                dat_fname='../dat/{0}_{1}_{2}_embedding_matrix.dat'.format(str(opt.embed_dim), opt.dataset,opt.add_num))\n",
        "            self.model = opt.model_class(embedding_matrix, opt).to(opt.device)\n",
        "\n",
        "        self.trainset = ABSADataset(opt.dataset_file['train'], tokenizer,self.opt)\n",
        "        self.testset = ABSADataset(opt.dataset_file['test'], tokenizer,self.opt,tt_model='test')\n",
        "        if 'vast' in opt.dataset:\n",
        "            self.valset = ABSADataset(opt.dataset_file['val'], tokenizer,self.opt,tt_model='test')\n",
        "        else:\n",
        "            assert 0 <= opt.valset_ratio < 1\n",
        "            if opt.valset_ratio > 0:\n",
        "                valset_len = int(len(self.trainset) * opt.valset_ratio)\n",
        "                self.trainset, self.valset = random_split(self.trainset, (len(self.trainset)-valset_len, valset_len))\n",
        "            else:\n",
        "                self.valset = self.testset\n",
        "        self.frontaddset = ABSADataset(opt.dataset_file['front_add'], tokenizer,self.opt,tt_model='add',ratio=self.opt.FAD_ratio)\n",
        "        self.reverseaddset = ABSADataset(opt.dataset_file['reverse_add'], tokenizer,self.opt,tt_model='add',ratio=self.opt.RAD_ratio)\n",
        "        self.trainset = ConcatDataset([self.trainset,self.frontaddset,self.reverseaddset])\n",
        "        # self.trainset = ConcatDataset([self.trainset,self.frontaddset])\n",
        "        print(len(self.trainset),len(self.testset),len(self.valset))\n",
        "\n",
        "\n",
        "        if opt.device.type == 'cuda':\n",
        "            logger.info('cuda memory allocated: {}'.format(torch.cuda.memory_allocated(device=opt.device.index)))\n",
        "        self._print_args()\n",
        "\n",
        "    def _print_args(self):\n",
        "        n_trainable_params, n_nontrainable_params = 0, 0\n",
        "        for p in self.model.parameters():\n",
        "            n_params = torch.prod(torch.tensor(p.shape))\n",
        "            if p.requires_grad:\n",
        "                n_trainable_params += n_params\n",
        "            else:\n",
        "                n_nontrainable_params += n_params\n",
        "        self.logger.info('> n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))\n",
        "        self.logger.info('> training arguments:')\n",
        "        for arg in vars(self.opt):\n",
        "            self.logger.info('>>> {0}: {1}'.format(arg, getattr(self.opt, arg)))\n",
        "\n",
        "    def _reset_params(self):\n",
        "        if 'bart' not in self.opt.model_name:\n",
        "            for child in self.model.children():\n",
        "                if type(child) != BertModel and type(child) != RobertaModel and type(child) != AutoModel:  # skip bert params !!\n",
        "                    for p in child.parameters():\n",
        "                        if p.requires_grad:\n",
        "                            if len(p.shape) > 1:\n",
        "                                self.opt.initializer(p)\n",
        "                            else:\n",
        "                                stdv = 1. / math.sqrt(p.shape[0])\n",
        "                                torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "        else:\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if 'bart' not in n:\n",
        "                    if p.requires_grad:\n",
        "                            if len(p.shape) > 1:\n",
        "                                self.opt.initializer(p)\n",
        "                            else:\n",
        "                                stdv = 1. / math.sqrt(p.shape[0])\n",
        "                                torch.nn.init.uniform_(p, a=-stdv, b=stdv)\n",
        "\n",
        "    def _train(self, criterion, optimizer, train_data_loader, val_data_loader,test_data_loader,bert_optimizer = None):\n",
        "        best_val = []\n",
        "        best_test = []\n",
        "        max_maf1a = 0\n",
        "        max_val_epoch = 0\n",
        "        # global_step = 0\n",
        "        path = None\n",
        "        if self.opt.log_step == -1:\n",
        "            self.opt.log_step = len(train_data_loader) // 3\n",
        "        ii_batch = 0\n",
        "        for i_epoch in range(self.opt.num_epoch):\n",
        "            self.logger.info('epoch: {}'.format(i_epoch))\n",
        "            n_correct, n_total, loss_total = 0, 0, 0\n",
        "            for i_batch, batch in enumerate(train_data_loader):\n",
        "                self.model.train()\n",
        "                # global_step += 1\n",
        "                optimizer.zero_grad()\n",
        "                if bert_optimizer != None:\n",
        "                    bert_optimizer.zero_grad()\n",
        "                inputs = [batch[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
        "                outputs = self.model(inputs)\n",
        "                targets = batch['polarity'].to(self.opt.device)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                if bert_optimizer != None:\n",
        "                    bert_optimizer.step()\n",
        "\n",
        "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
        "                n_total += len(outputs)\n",
        "                loss_total += loss.item() * len(outputs)\n",
        "                ii_batch += 1\n",
        "                if ii_batch % self.opt.log_step == 0:\n",
        "                    train_acc = n_correct / n_total\n",
        "                    train_loss = loss_total / n_total\n",
        "                    self.logger.info(f'{ii_batch}/{len(train_data_loader)}\\ttrain_loss: {train_loss}\\ttrain_acc: {round(train_acc * 100,2)}')\n",
        "                    val_acc, val_f1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a = self._evaluate_acc_f1(val_data_loader)\n",
        "                    self.logger.info(f'Val: ma_f1: {round(val_f1*100,2)}\\tacc: {round(val_acc*100,2)}\\tavg_f1: {round(avg_f1*100,2)}\\tma_all_f1: {round(maf1a*100,2)}\\tmi_all_f1: {round(mif1a*100,2)}\\tfavor_f1: {round(f_f1*100,2)}\\tagainst_f1: {round(a_f1*100,2)}\\tnone_f1: {round(n_f1*100,2)}')\n",
        "                    if 'vast' in self.opt.dataset:\n",
        "                        sign_fa = maf1a\n",
        "                    else:\n",
        "                        sign_fa = val_f1\n",
        "                    if sign_fa > max_maf1a:\n",
        "                        best_val = [val_acc,val_f1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a,i_epoch]\n",
        "                        max_maf1a = sign_fa\n",
        "                        max_val_epoch = i_epoch\n",
        "                        test_acc, test_f1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a = self._evaluate_acc_f1(test_data_loader)\n",
        "                        best_test = [test_acc, test_f1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a,i_epoch]\n",
        "                        self.logger.info(f'Test: ma_f1: {round(test_f1*100,2)}\\tacc: {round(test_acc*100,2)}\\tavg_f1: {round(avg_f1*100,2)}\\tma_all_f1: {round(maf1a*100,2)}\\tmi_all_f1: {round(mif1a*100,2)}\\tfavor_f1: {round(f_f1*100,2)}\\tagainst_f1: {round(a_f1*100,2)}\\tnone_f1: {round(n_f1*100,2)}')\n",
        "                        if self.opt.save_model:\n",
        "                            path = f'../state_dict/edda_{self.opt.model_name}_{self.opt.dataset}_{self.opt.input_type}_{self.opt.label_ratio}_{self.opt.FAD_ratio}_{self.opt.RAD_ratio}'\n",
        "                            torch.save(self.model.state_dict(), path)\n",
        "            if i_epoch - max_val_epoch >= self.opt.patience:\n",
        "                print('>> early stop.')\n",
        "                break\n",
        "        self.logger.info(f'Best_test: epoch:{best_test[-1]}\\tma_f1: {round(best_test[1]*100,2)}\\tacc: {round(best_test[0]*100,2)}\\tavg_f1: {round(best_test[2]*100,2)}\\tma_all_f1: {round(best_test[6]*100,2)}\\tmi_all_f1: {round(best_test[7]*100,2)}\\tfavor_f1: {round(best_test[3]*100,2)}\\tagainst_f1: {round(best_test[4]*100,2)}\\tnone_f1: {round(best_test[5]*100,2)}')\n",
        "        self.best_logger.info(f'Best_test: epoch:{best_test[-1]}\\tma_f1: {round(best_test[1]*100,2)}\\tacc: {round(best_test[0]*100,2)}\\tavg_f1: {round(best_test[2]*100,2)}\\tma_all_f1: {round(best_test[6]*100,2)}\\tmi_all_f1: {round(best_test[7]*100,2)}\\tfavor_f1: {round(best_test[3]*100,2)}\\tagainst_f1: {round(best_test[4]*100,2)}\\tnone_f1: {round(best_test[5]*100,2)}')\n",
        "        self.logger.info(f'Best_val: epoch:{best_val[-1]}\\tma_f1: {round(best_val[1]*100,2)}\\tacc: {round(best_val[0]*100,2)}\\tavg_f1: {round(best_val[2]*100,2)}\\tma_all_f1: {round(best_val[6]*100,2)}\\tmi_all_f1: {round(best_val[7]*100,2)}\\tfavor_f1: {round(best_val[3]*100,2)}\\tagainst_f1: {round(best_val[4]*100,2)}\\tnone_f1: {round(best_val[5]*100,2)}')\n",
        "# =====\n",
        "        if self.opt.save_model:\n",
        "            torch.save(self.model.state_dict(), f'../state_dict/edda_last_{self.opt.model_name}_{self.opt.dataset}_{self.opt.input_type}_{self.opt.label_ratio}_{self.opt.FAD_ratio}_{self.opt.RAD_ratio}')\n",
        "        return path\n",
        "\n",
        "    def _evaluate_acc_f1(self, data_loader):\n",
        "        t_targets_all, t_outputs_all = None, None\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i_batch, t_batch in enumerate(data_loader):\n",
        "                t_inputs = [t_batch[col].to(self.opt.device) for col in self.opt.inputs_cols]\n",
        "                t_outputs = self.model(t_inputs)\n",
        "                t_targets = t_batch['polarity'].to(self.opt.device)\n",
        "                if t_targets_all is None:\n",
        "                    t_targets_all = t_targets.cpu()\n",
        "                    t_outputs_all = t_outputs.cpu()\n",
        "                else:\n",
        "                    t_targets_all = torch.cat((t_targets_all, t_targets.cpu()), dim=0)\n",
        "                    t_outputs_all = torch.cat((t_outputs_all, t_outputs.cpu()), dim=0)\n",
        "        acc = metrics.accuracy_score(t_targets_all, torch.argmax(t_outputs_all, -1))\n",
        "        maf1a = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='macro')\n",
        "        mif1a = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='micro')\n",
        "        maf1 = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='macro',labels=[0,1])\n",
        "        mif1 = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='micro',labels=[0,1])\n",
        "        f_f1 = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='macro',labels=[1])\n",
        "        a_f1 = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='macro',labels=[0])\n",
        "        n_f1 = metrics.f1_score(t_targets_all, torch.argmax(t_outputs_all, -1), average='macro',labels=[2])\n",
        "        avg_f1 = (mif1 + maf1)/2\n",
        "        return acc, maf1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        # print('='*30)\n",
        "        # for child in self.model.children():\n",
        "        #     for n,p in child.named_parameters():\n",
        "        #         print(n)\n",
        "        #         print(n.startswith('bart.encoder.layer'))\n",
        "\n",
        "        #     print(type(child) == \"<class 'models.bart.Encoder'>\")\n",
        "        #     print(type(child) == 'models.bart.Encoder')\n",
        "        _params = [\n",
        "            # {'params': [p for p in child.parameters()]} for child in self.model.children() if type(child) != BertModel and type(child) != RobertaModel and type(child) != AutoModel\n",
        "            {'params': [p for n,p in child.named_parameters()],'lr':self.opt.lr} for child in self.model.children() if type(child) != BertModel and type(child) != RobertaModel and type(child) != AutoModel\n",
        "        ]\n",
        "        # optimizer = self.opt.optimizer(_params, lr=self.opt.lr,weight_decay=0.0001)\n",
        "\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        if 'bert' in self.opt.model_name:\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in self.model.bert.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-5,'lr':self.opt.bert_lr},\n",
        "                {'params': [p for n, p in self.model.bert.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,'lr':self.opt.bert_lr},\n",
        "            ]\n",
        "            if self.opt.model_name == 'bert_rn':\n",
        "                new_optimizer_parameters = [\n",
        "                    {'params': [p for n, p in self.model.bert2.named_parameters() if not any(nd in n for nd in no_decay)],'lr': self.opt.bert_lr, 'weight_decay': 1e-5},\n",
        "                    {'params': [p for n, p in self.model.bert2.named_parameters() if any(nd in n for nd in no_decay)], 'lr': self.opt.bert_lr, 'weight_decay': 0.0},\n",
        "                ]\n",
        "                optimizer_grouped_parameters += new_optimizer_parameters\n",
        "            optimizer_grouped_parameters += _params\n",
        "        elif 'bart' in self.opt.model_name:\n",
        "            for n, p in self.model.named_parameters():\n",
        "                if \"bart.shared.weight\" in n or \"bart.encoder.embed\" in n:\n",
        "                    p.requires_grad = False\n",
        "            optimizer_grouped_parameters = [\n",
        "                {'params': [p for n, p in self.model.named_parameters() if n.startswith('bart.encoder.layer')] , 'lr': self.opt.bert_lr},\n",
        "                {'params': [p for n, p in self.model.named_parameters() if not n.startswith('bart.encoder.layer')] , 'lr': self.opt.lr},\n",
        "                ]\n",
        "        else:\n",
        "            optimizer_grouped_parameters = _params\n",
        "\n",
        "        optimizer = self.opt.optimizer(optimizer_grouped_parameters)\n",
        "\n",
        "\n",
        "        train_data_loader = DataLoader(dataset=self.trainset, batch_size=self.opt.batch_size, shuffle=True)\n",
        "        test_data_loader = DataLoader(dataset=self.testset, batch_size=self.opt.batch_size, shuffle=False)\n",
        "        val_data_loader = DataLoader(dataset=self.valset, batch_size=self.opt.batch_size, shuffle=False)\n",
        "\n",
        "        self._reset_params()\n",
        "        print(len(self.testset))\n",
        "        best_model_path = self._train(criterion, optimizer, train_data_loader, val_data_loader,test_data_loader)\n",
        "        if self.opt.save_model:\n",
        "            self.model.load_state_dict(torch.load(best_model_path))\n",
        "            test_acc, test_f1,avg_f1,f_f1,a_f1,n_f1,maf1a,mif1a = self._evaluate_acc_f1(test_data_loader)\n",
        "            self.logger.info(f'Best_test: ma_f1: {round(test_f1*100,2)}\\tacc: {round(test_acc*100,2)}\\tavg_f1: {round(avg_f1*100,2)}\\tma_all_f1: {round(maf1a*100,2)}\\tmi_all_f1: {round(mif1a*100,2)}\\tfavor_f1: {round(f_f1*100,2)}\\tagainst_f1: {round(a_f1*100,2)}\\tnone_f1: {round(n_f1*100,2)}')\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Hyper Parameters\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--model_name', default='lstm', type=str)\n",
        "    parser.add_argument('--input_type', default='tt', type=str)\n",
        "    parser.add_argument('--dataset', default='-dt', type=str, help='twitter, restaurant, laptop')\n",
        "    parser.add_argument('--optimizer', default='adamw', type=str)\n",
        "    parser.add_argument('--initializer', default='xavier_uniform_', type=str)\n",
        "    parser.add_argument('--lr', default=1e-3, type=float, help='try 5e-5, 2e-5 for BERT, 1e-3 for others')\n",
        "    parser.add_argument('--bert_lr', default=2e-5, type=float, help='try 5e-5, 2e-5 for BERT, 1e-3 for others')\n",
        "    parser.add_argument('--dropout', default=0.2, type=float)\n",
        "    parser.add_argument('--num_epoch', default=50, type=int, help='try larger number for non-BERT models')\n",
        "    parser.add_argument('--batch_size', default=16, type=int, help='try 16, 32, 64 for BERT models')\n",
        "    parser.add_argument('--log_step', default=-1, type=int)\n",
        "    # parser.add_argument('--pretrained_name', default='bert-base-uncased', type=str)\n",
        "    parser.add_argument('--max_seq_len', default=80, type=int)\n",
        "    parser.add_argument('--polarities_dim', default=3, type=int)\n",
        "    parser.add_argument('--patience', default=10, type=int)\n",
        "    parser.add_argument('--device', default='cuda:0', type=str, help='e.g. cuda:0')\n",
        "    parser.add_argument('--save_model', default=False, type=bool)\n",
        "    parser.add_argument('--seed', default=2023, type=int, help='set seed for reproducibility')\n",
        "    parser.add_argument('--add_num', default=0, type=int, help='front_add and reverse_add')\n",
        "    parser.add_argument('--valset_ratio', default=0.15, type=float, help='set ratio between 0 and 1 for validation support')\n",
        "    parser.add_argument('--FAD_ratio', default=1, type=float, help='set ratio between 0 and 1 for front data augmention')\n",
        "    parser.add_argument('--RAD_ratio', default=1, type=float, help='set ratio between 0 and 1 for reverse data augmention')\n",
        "    parser.add_argument('--label_ratio', default=1, type=float, help='set ratio between 0 and 1 for label')\n",
        "    parser.add_argument('--embed_dim', default=300, type=int)\n",
        "    parser.add_argument('--hidden_dim', default=300, type=int)\n",
        "    parser.add_argument('--lambadd', default=0.2, type=float)\n",
        "    opt = parser.parse_args()\n",
        "    Model2Path = {\n",
        "        'bert': 'bert-base-uncased',\n",
        "        'bert_rn': 'bert-base-uncased',\n",
        "        'roberta': 'roberta-base',\n",
        "        'bart': 'facebook/bart-large-mnli',\n",
        "        'bart_rn': 'facebook/bart-large-mnli',\n",
        "    }\n",
        "    if opt.model_name in Model2Path:\n",
        "        opt.pretrained_name = Model2Path[opt.model_name]\n",
        "    if 'bert' in opt.model_name:\n",
        "        opt.patience = 4\n",
        "        # opt.num_epoch = 10\n",
        "    elif 'bart' in opt.model_name:\n",
        "        opt.num_epoch = 4\n",
        "    else:\n",
        "        opt.patience = 5\n",
        "        opt.num_epoch = 25\n",
        "    if opt.seed is not None:\n",
        "        random.seed(opt.seed)\n",
        "        numpy.random.seed(opt.seed)\n",
        "        torch.manual_seed(opt.seed)\n",
        "        torch.cuda.manual_seed(opt.seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        os.environ['PYTHONHASHSEED'] = str(opt.seed)\n",
        "\n",
        "    model_classes = {\n",
        "        'lstm': LSTM,\n",
        "        'bert': BERT_SPC,\n",
        "        'td_lstm': TD_LSTM,\n",
        "        'tc_lstm': TC_LSTM,\n",
        "        'atae_lstm': ATAE_LSTM,\n",
        "        'ian': IAN,\n",
        "        'memnet': MemNet,\n",
        "        'ram': RAM,\n",
        "        'cabasc': Cabasc,\n",
        "        'tnet_lf': TNet_LF,\n",
        "        'aoa': AOA,\n",
        "        'mgan': MGAN,\n",
        "        'asgcn': ASGCN,\n",
        "        'bert_spc': BERT_SPC,\n",
        "        'bert_rn': BERT_RN,\n",
        "        'roberta':BERT_SPC,\n",
        "    }\n",
        "    def dataset_files(data_target):\n",
        "        if '-' not in data_target:\n",
        "            # return {\n",
        "            #     'test': f'{BASEPATH}/sem16/IF-Then/{data_target}/test.csv',\n",
        "            #     'train': f'{BASEPATH}/sem16/IF-Then/{data_target}/train.csv',\n",
        "            #     'val': f'{BASEPATH}/sem16/IF-Then/{data_target}/val.csv',\n",
        "            #     'front_add': f'{BASEPATH}/front_add/{data_target}.csv',\n",
        "            #     'reverse_add': f'{BASEPATH}/reverse_add/{data_target}.csv',\n",
        "            # }\n",
        "\n",
        "            return {\n",
        "                'train': f'{BASEPATH}/llam-datasets/sem16/IF-Then/{data_target}/train_10.csv',\n",
        "                'test': f'{BASEPATH}/llam-datasets/sem16/IF-Then/{data_target}/test.csv',\n",
        "                'val': f'{BASEPATH}/llam-datasets/sem16/IF-Then/{data_target}/val.csv',\n",
        "                # 'front_add': f'/home/dingdaijun/data_list/dingdaijun/code/OpenStance/data/VAST/vast_mask_sentence.csv',\n",
        "                # 'reverse_add': f'/home/dingdaijun/data_list/dingdaijun/code/OpenStance/data/VAST/vast_mask_topic.csv',\n",
        "                'front_add': f'{BASEPATH}/llam-datasets/front_add/vast2/{data_target}.csv',\n",
        "                'reverse_add': f'{BASEPATH}/llam-datasets/reverse_add/vast2/{data_target}.csv',\n",
        "            }\n",
        "        else:\n",
        "            source_target, destin_target = data_target.split('-')\n",
        "            if source_target != '' and destin_target != '':\n",
        "                return {\n",
        "                    'train': f'{BASEPATH}/sem16/IF-Then/{source_target}/{source_target}.csv',\n",
        "                    'test': f'{BASEPATH}/sem16/IF-Then/{destin_target}/test.csv',\n",
        "                    'front_add': f'{BASEPATH}/front_add/{source_target}.csv',\n",
        "                    'reverse_add': f'{BASEPATH}/reverse_add/{source_target}.csv',\n",
        "                }\n",
        "            else:\n",
        "                all_target = ['dt','la','fm','hc']\n",
        "                destin_target = next(filter(lambda x: x != '', [source_target,destin_target]))\n",
        "                all_target.remove(destin_target)\n",
        "                assert len(all_target) == 3\n",
        "                return {\n",
        "                    'train': [\n",
        "                        f'{BASEPATH}/llam-datasets/sem16/IF-Then/{all_target[1]}/{all_target[1]}.csv',\n",
        "                        f'{BASEPATH}/llam-datasets/sem16/IF-Then/{all_target[0]}/{all_target[0]}.csv',\n",
        "                        f'{BASEPATH}/llam-datasets/sem16/IF-Then/{all_target[2]}/{all_target[2]}.csv',\n",
        "                        ],\n",
        "                    'test': f'{BASEPATH}/sem16/IF-Then/{destin_target}/test.csv',\n",
        "                    'front_add': [\n",
        "                        f'{BASEPATH}/front_add/{all_target[0]}.csv',\n",
        "                        f'{BASEPATH}/front_add/{all_target[1]}.csv',\n",
        "                        f'{BASEPATH}/front_add/{all_target[2]}.csv',\n",
        "                        ],\n",
        "                    'reverse_add': [\n",
        "                        f'{BASEPATH}/reverse_add/{all_target[0]}.csv',\n",
        "                        f'{BASEPATH}/reverse_add/{all_target[1]}.csv',\n",
        "                        f'{BASEPATH}/reverse_add/{all_target[2]}.csv',\n",
        "                        ],\n",
        "                }\n",
        "\n",
        "    input_colses = {\n",
        "        'td_lstm': ['left_with_aspect_indices', 'right_with_aspect_indices'],\n",
        "        'tc_lstm': ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices'],\n",
        "        'atae_lstm': ['text_indices', 'aspect_indices'],\n",
        "        'ian': ['text_indices', 'aspect_indices'],\n",
        "        'memnet': ['context_indices', 'aspect_indices'],\n",
        "        'ram': ['text_indices', 'aspect_indices', 'left_indices'],\n",
        "        'cabasc': ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices'],\n",
        "        'tnet_lf': ['text_indices', 'aspect_indices', 'aspect_boundary'],\n",
        "        'aoa': ['text_indices', 'aspect_indices'],\n",
        "        'mgan': ['text_indices', 'aspect_indices', 'left_indices'],\n",
        "        'asgcn': ['text_indices', 'aspect_indices', 'left_indices', 'dependency_graph'],\n",
        "        'bert_spc': ['bert_text_target_indices'],\n",
        "        'bert_rn_tt': ['bert_text_target_inputs','bert_text_target_type','bert_text_target_mask','bert_reason_inputs','bert_reason_type','bert_reason_mask','polarity'],\n",
        "    }\n",
        "    initializers = {\n",
        "        'xavier_uniform_': torch.nn.init.xavier_uniform_,\n",
        "        'xavier_normal_': torch.nn.init.xavier_normal_,\n",
        "        'orthogonal_': torch.nn.init.orthogonal_,\n",
        "    }\n",
        "    optimizers = {\n",
        "        'adadelta': torch.optim.Adadelta,  # default lr=1.0\n",
        "        'adagrad': torch.optim.Adagrad,  # default lr=0.01\n",
        "        'adam': torch.optim.Adam,  # default lr=0.001\n",
        "        'adamax': torch.optim.Adamax,  # default lr=0.002\n",
        "        'asgd': torch.optim.ASGD,  # default lr=0.01\n",
        "        'rmsprop': torch.optim.RMSprop,  # default lr=0.01\n",
        "        'sgd': torch.optim.SGD,\n",
        "        'adamw':AdamW,\n",
        "    }\n",
        "    opt.model_class = model_classes[opt.model_name]\n",
        "    opt.dataset_file = dataset_files(opt.dataset)\n",
        "    opt.inputs_cols = input_colses[f'{opt.model_name}_{opt.input_type}']\n",
        "    opt.initializer = initializers[opt.initializer]\n",
        "    opt.optimizer = optimizers[opt.optimizer]\n",
        "    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \\\n",
        "        if opt.device is None else torch.device(opt.device)\n",
        "\n",
        "    ins = Instructor(opt)\n",
        "    ins.run()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}